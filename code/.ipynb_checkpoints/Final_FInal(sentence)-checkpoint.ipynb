{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>Sentence_ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Gene1|Gene1_ID</th>\n",
       "      <th>Gene1_Index(start|end)</th>\n",
       "      <th>Gene2|Gene2_ID</th>\n",
       "      <th>Gene2_Index(start|end)</th>\n",
       "      <th>RE_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24019522</td>\n",
       "      <td>S7</td>\n",
       "      <td>Notably, all of the H3K36-specific methyltrans...</td>\n",
       "      <td>ASH1L|55870</td>\n",
       "      <td>65|70</td>\n",
       "      <td>HYPB|29072</td>\n",
       "      <td>72|76</td>\n",
       "      <td>NoRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24019522</td>\n",
       "      <td>S7</td>\n",
       "      <td>Notably, all of the H3K36-specific methyltrans...</td>\n",
       "      <td>ASH1L|55870</td>\n",
       "      <td>65|70</td>\n",
       "      <td>NSD1|64324</td>\n",
       "      <td>78|82</td>\n",
       "      <td>NoRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24019522</td>\n",
       "      <td>S7</td>\n",
       "      <td>Notably, all of the H3K36-specific methyltrans...</td>\n",
       "      <td>ASH1L|55870</td>\n",
       "      <td>65|70</td>\n",
       "      <td>NSD2|7468</td>\n",
       "      <td>88|92</td>\n",
       "      <td>NoRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24019522</td>\n",
       "      <td>S7</td>\n",
       "      <td>Notably, all of the H3K36-specific methyltrans...</td>\n",
       "      <td>ASH1L|55870</td>\n",
       "      <td>65|70</td>\n",
       "      <td>Pr-Set7|387893</td>\n",
       "      <td>189|196</td>\n",
       "      <td>NoRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24019522</td>\n",
       "      <td>S7</td>\n",
       "      <td>Notably, all of the H3K36-specific methyltrans...</td>\n",
       "      <td>ASH1L|55870</td>\n",
       "      <td>65|70</td>\n",
       "      <td>H2A|8337</td>\n",
       "      <td>113|116</td>\n",
       "      <td>Negative_Regulation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PMID Sentence_ID                                           Sentence  \\\n",
       "0  24019522          S7  Notably, all of the H3K36-specific methyltrans...   \n",
       "1  24019522          S7  Notably, all of the H3K36-specific methyltrans...   \n",
       "2  24019522          S7  Notably, all of the H3K36-specific methyltrans...   \n",
       "3  24019522          S7  Notably, all of the H3K36-specific methyltrans...   \n",
       "4  24019522          S7  Notably, all of the H3K36-specific methyltrans...   \n",
       "\n",
       "  Gene1|Gene1_ID Gene1_Index(start|end)  Gene2|Gene2_ID  \\\n",
       "0    ASH1L|55870                  65|70      HYPB|29072   \n",
       "1    ASH1L|55870                  65|70      NSD1|64324   \n",
       "2    ASH1L|55870                  65|70       NSD2|7468   \n",
       "3    ASH1L|55870                  65|70  Pr-Set7|387893   \n",
       "4    ASH1L|55870                  65|70        H2A|8337   \n",
       "\n",
       "  Gene2_Index(start|end)              RE_Type  \n",
       "0                  72|76                 NoRE  \n",
       "1                  78|82                 NoRE  \n",
       "2                  88|92                 NoRE  \n",
       "3                189|196                 NoRE  \n",
       "4                113|116  Negative_Regulation  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "filepath = 'train.tsv'\n",
    "filepath1 = 'dev_part1.tsv'\n",
    "df = pd.read_csv(filepath, sep='\\t', encoding = \"ISO-8859-1\")\n",
    "wf = pd.read_csv(filepath1, sep='\\t', encoding = \"ISO-8859-1\")\n",
    "\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['Sentence']\n",
    "target = df['RE_Type']\n",
    "PMID = df['PMID']\n",
    "Gene1 = df['Gene1|Gene1_ID']\n",
    "Gene2 = df['Gene2|Gene2_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences.str.replace('/', ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.分句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sen1=[]\n",
    "for i in sentences:\n",
    "    sen1.append(sent_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Taken together, these observations indicate that PLD1 is activated by PLCg,PKCa signaling and stimulate Bcl-2 expression through PLA2,Cox2,EP4,PKA,p38MAPK,CREB during neuronal differentiation of rat neural stem cells.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen1[375]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = []\n",
    "for i in sentences:\n",
    "    sen.append(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = []\n",
    "english_punctuations = [',', '/']\n",
    "for i in range(0, len(sen)):\n",
    "    for j in sen[i]:\n",
    "        if j not in english_stopwords: # 過濾停用詞\n",
    "            if j not in english_punctuations: # 過濾標點符號\n",
    "                temp1.append(j)\n",
    "    sen[i] = temp1\n",
    "    temp1 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將Gene1進行分割 只留下純文字的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene11 = []\n",
    "gene22 = []\n",
    "for i in range(0, len(Gene1)):\n",
    "    gene11.append(Gene1[i].split('|')[0])\n",
    "    gene22.append(Gene2[i].split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene1 = []\n",
    "gene2 = []\n",
    "for i in range(0, len(Gene1)):\n",
    "    gene1.append(gene11[i].split('/')[0])\n",
    "    gene2.append(gene22[i].split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge1 = []\n",
    "for i in gene1:\n",
    "    ge1.append(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge2 = []\n",
    "for i in gene2:\n",
    "    ge2.append(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = []\n",
    "for i in range(0, len(ge1)):\n",
    "    if len(ge1[i]) > 1:\n",
    "        g1.append(\"\".join(ge1[i][0]))\n",
    "    else:\n",
    "        g1.append(\"\".join(ge1[i]))\n",
    "\n",
    "g2 = []\n",
    "for i in range(0, len(ge2)):\n",
    "    if len(ge2[i]) > 1:\n",
    "        g2.append(\"\".join(ge2[i][0]))\n",
    "    else:\n",
    "        g2.append(\"\".join(ge2[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將target轉化成數字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = dict()\n",
    "label_set = set()\n",
    "for l in target:\n",
    "    label_set.add(l)\n",
    "for ll in label_set:\n",
    "    label_dict[ll] = len(label_dict)\n",
    "label_dict_inverse = {v : k for k, v in label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "target_int = [label_dict[ll] for ll in target]\n",
    "target_onehot = to_categorical(target_int, num_classes=len(label_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = []\n",
    "train_list = []\n",
    "for i in range(0, len(target)):\n",
    "    word.append([Gene1[i], Gene2[i]])\n",
    "    train_list.append([word[i], target_onehot[i]])\n",
    "train_list = np.array(train_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "進行語料庫的建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sen, size = 300, window = 5, min_count = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此函數是用來找前十名最接近輸入詞的詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(w2v_model, words, topn=10):\n",
    "    similar_df = pd.DataFrame()\n",
    "    for word in words:\n",
    "        try:\n",
    "            similar_words = pd.DataFrame(w2v_model.wv.most_similar(word, topn=topn), columns=[word, 'cos'])\n",
    "            similar_df = pd.concat([similar_df, similar_words], axis=1)\n",
    "        except:\n",
    "            print(word, \"not found in Word2Vec model!\")\n",
    "    return similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Notably</th>\n",
       "      <th>cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CLL-induced</td>\n",
       "      <td>0.972134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tumor-only</td>\n",
       "      <td>0.967945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FFPE</td>\n",
       "      <td>0.967863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marfan</td>\n",
       "      <td>0.966250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>panel-based</td>\n",
       "      <td>0.963051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ugt1</td>\n",
       "      <td>0.962769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>side</td>\n",
       "      <td>0.962317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>scenarios</td>\n",
       "      <td>0.962288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vpr</td>\n",
       "      <td>0.961108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>behaviour</td>\n",
       "      <td>0.961031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Notably       cos\n",
       "0  CLL-induced  0.972134\n",
       "1   tumor-only  0.967945\n",
       "2         FFPE  0.967863\n",
       "3       Marfan  0.966250\n",
       "4  panel-based  0.963051\n",
       "5         Ugt1  0.962769\n",
       "6         side  0.962317\n",
       "7    scenarios  0.962288\n",
       "8          vpr  0.961108\n",
       "9    behaviour  0.961031"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(model, ['Notably'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此函數用來找兩個詞之間的關係（餘弦值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word1, word2):\n",
    "    try:\n",
    "        val = model.similarity(word1, word2)\n",
    "    except:\n",
    "        val = 0.5\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算每一組Gene1和Gene2的關係值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harry02261112/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "cos = []\n",
    "for i in range(0, len(g1)):\n",
    "    cos.append(similarity(g1[i], g2[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7371127709073099 0.869797953293099\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble, preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = cos\n",
    "X = np.array(X)\n",
    "y = target_onehot\n",
    "\n",
    "##target_new\n",
    "\n",
    "y = np.array(y)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# 建立 random forest 模型\n",
    "forest = ensemble.RandomForestClassifier(n_estimators = 100)\n",
    "forest_fit = forest.fit(train_X.reshape(-1, 1), train_y)\n",
    "\n",
    "# 預測\"\n",
    "test_y_predicted = forest.predict(test_X.reshape(-1, 1))\n",
    "train_y_predicted = forest.predict(train_X.reshape(-1, 1))\n",
    "\n",
    "# 績效\n",
    "accuracy_test_forest = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "accuracy_train_forest = metrics.accuracy_score(train_y, train_y_predicted)\n",
    "print(accuracy_test_forest,accuracy_train_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7371127709073099\n"
     ]
    }
   ],
   "source": [
    "y_null = []\n",
    "for i in range(0, len(test_y)):\n",
    "    y_null.append('NoRE')\n",
    "    \n",
    "accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests: \n",
      "Accuracy of Random Forests is:  0.7167722850035321\n",
      "\n",
      "\n",
      "knn:\n",
      "0.7733443107853635\n",
      "Accuracy of Knn is:  0.7733443107853635\n",
      "\n",
      "\n",
      "SVM:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harry02261112/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM is:  0.8185302571581385\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forests: \")\n",
    "forest = ensemble.RandomForestClassifier(n_estimators = 100)\n",
    "accuracy = cross_val_score(forest, X.reshape(-1, 1),y,scoring='accuracy', cv = 5)\n",
    "print(\"Accuracy of Random Forests is: \" , accuracy.mean())\n",
    "print(\"\\n\\nknn:\")\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "scores = cross_val_score(knn,X.reshape(-1, 1),y,cv=5,scoring='accuracy')\n",
    "print(scores.mean())\n",
    "print(\"Accuracy of Knn is: \" , scores.mean())\n",
    "print(\"\\n\\nSVM:\")\n",
    "y = target\n",
    "svm_class = SVC(kernel = 'linear')\n",
    "accuracy = cross_val_score(svm_class, X.reshape(-1, 1), y, scoring='accuracy', cv = 5).mean()\n",
    "print(\"Accuracy of SVM is: \" , accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harry02261112/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "x1 = wf['Gene1|Gene1_ID']\n",
    "x2 = wf['Gene2|Gene2_ID']\n",
    "xx1 = []\n",
    "xx2 = []\n",
    "for i in range(0, len(x1)):\n",
    "    xx1.append(x1[i].split('|')[0])\n",
    "    xx2.append(x2[i].split('|')[0])\n",
    "cos_dev = []\n",
    "for i in range(0, len(x1)):\n",
    "    cos_dev.append(similarity(xx1[i], xx2[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cos\n",
    "X = np.array(X)\n",
    "y = target\n",
    "y = np.array(y)\n",
    "\n",
    "# 建立SVM模型\n",
    "svm_class = SVC(kernel = 'linear')\n",
    "svm_class.fit(X.reshape(-1, 1), y)\n",
    "\n",
    "# 預測\n",
    "cos_dev = np.array(cos_dev)\n",
    "predicted = svm_class.predict(cos_dev.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression() #初始化 LR model\n",
    "X = cos\n",
    "X = np.array(X)\n",
    "y = target\n",
    "y = np.array(y)\n",
    "\n",
    "classifier.fit(X.reshape(-1, 1), y)\n",
    "cos_dev = np.array(cos_dev)\n",
    "predicted = classifier.predict(cos_dev.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cos\n",
    "#vector_plus\n",
    "X = np.array(X)\n",
    "y = target\n",
    "y = np.array(y)\n",
    "\n",
    "# 建立SVM模型\n",
    "svm_class = SVC(kernel = 'linear')\n",
    "svm_class.fit(X.reshape(-1, 1), y)#.reshape(-1, 1)\n",
    "\n",
    "# 預測\n",
    "cos_dev = np.array(cos_dev)\n",
    "predicted = svm_class.predict(cos_dev.reshape(-1, 1))#.reshape(-1, 1)\n",
    "#vec_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k fold cross validation \n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "kf = KFold(n_splits=5)\n",
    "X = train_X\n",
    "y = train_y\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "validation_acc_forest = np.array([])\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train_forest, X_test_forest = X[train_index], X[test_index]\n",
    "    y_train_forest, y_test_forest = y[train_index], y[test_index]\n",
    "    forest = ensemble.RandomForestClassifier(n_estimators = 100)\n",
    "    forest_fit = forest.fit(X_train_forest.reshape(-1, 1), y_train_forest)\n",
    "    y_test_predicted_forest = forest.predict(X_test_forest.reshape(-1, 1))\n",
    "    accuracy_forest = metrics.accuracy_score(y_test_forest, y_test_predicted_forest)\n",
    "    validation_acc_forest = np.append(validation_acc_forest,round(accuracy_forest,2))\n",
    "print(validation_acc_forest)\n",
    "val_accuracy_forest = round(np.mean(validation_acc_forest),2)\n",
    "print(\"The mean accuracy of the random forest validations :\",val_accuracy_forest)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# KNN\n",
    "\n",
    "validation_acc_knn = np.array([])\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train_knn, X_test_knn = X[train_index], X[test_index]\n",
    "    y_train_knn, y_test_knn = y[train_index], y[test_index]\n",
    "    knn = KNeighborsClassifier(n_neighbors=10)\n",
    "    knn.fit(X_train_knn.reshape(-1, 1),y_train_knn)\n",
    "    y_test_predicted_knn = knn.predict(X_test_knn.reshape(-1, 1))\n",
    "    accuracy_knn = metrics.accuracy_score(y_test_knn, y_test_predicted_knn)\n",
    "    validation_acc_knn = np.append(validation_acc_knn,round(accuracy_knn,2))\n",
    "print(validation_acc_knn)\n",
    "val_accuracy_knn = round(np.mean(validation_acc_knn),2)\n",
    "print(\"The mean accuracy of the knn validations :\",val_accuracy_knn)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# SVM\n",
    "\n",
    "validation_acc_svm = np.array([])\n",
    "y = np.array(target_int)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train_svm, X_test_svm = X[train_index], X[test_index]\n",
    "    y_train_svm, y_test_svm = y[train_index], y[test_index]\n",
    "    svm = SVR(kernel = 'linear')\n",
    "    svm.fit(X_train_svm.reshape(-1, 1),y_train_svm)\n",
    "    y_test_predicted_svm = svm.predict(X_test_svm.reshape(-1, 1))\n",
    "    y_test_predicted_svm_one_hot = to_categorical(y_test_predicted_svm, num_classes=len(label_dict))\n",
    "    y_test_svm_one_hot = to_categorical(y_test_svm, num_classes=len(label_dict))\n",
    "    accuracy_svm = metrics.accuracy_score(y_test_svm_one_hot, y_test_predicted_svm_one_hot)\n",
    "    validation_acc_svm = np.append(validation_acc_svm,round(accuracy_svm*350,2))\n",
    "print(validation_acc_svm)\n",
    "val_accuracy_svm = round(np.mean(validation_acc_svm),2)\n",
    "print(\"The mean accuracy of the svm validations :\",val_accuracy_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "寫入檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = pd.DataFrame({'RE_Type':predicted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = pd.concat([wf, pre], sort = False, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.to_csv(\"prediction_sentence.tsv\", encoding = 'utf-8', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "plt.figure(figsize=(25, 25))\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
